import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, explode, split, count, date_format, lit, row_number
from pyspark.sql.window import Window
import pyspark.sql.functions as F
import logging

# Configuración de logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("NewsDataProcessing")

# Crear la sesión de Spark
spark = SparkSession.builder \
    .appName("NewsDataProcessing") \
    .getOrCreate()

# Rutas de entrada y salida
input_path = "s3://spaceflight-data-pipeline/raw"
output_path = "s3://spaceflight-data-pipeline/processed"

logger.info("Leyendo datos desde S3...")
# Leer datos desde S3
data_df = spark.read.json(f"{input_path}/*.json")

# Extraer la clave 'results' para aplanar el DataFrame
results_df = data_df.selectExpr("explode(results) as result").select("result.*")

# Limpieza y selección de columnas relevantes
cleaned_df = results_df.select(
    "id", 
    "title", 
    "summary", 
    date_format(col("published_at"), "yyyy-MM-dd").alias("published_date"),
    lower(col("news_site")).alias("name"),
    "url"
)

logger.info("Procesando datos para análisis de contenido y tendencias...")
### ------------------ Análisis de contenido y tendencias ------------------ ###

# Análisis de contenido: Extracción de palabras clave
keywords_df = cleaned_df.withColumn("keywords", explode(split(col("summary"), "\\s+"))) \
    .groupBy("keywords") \
    .agg(count("keywords").alias("count")) \
    .orderBy(F.desc("count"))

# Análisis de tendencias: Conteo de artículos por fecha
trends_df = cleaned_df.groupBy("published_date") \
    .agg(count("id").alias("article_count")) \
    .orderBy("published_date")

logger.info("Guardando resultados en S3 (Parquet)...")
# Guardar resultados en S3 en formato Parquet
keywords_df.write.mode("overwrite").parquet(f"{output_path}/keywords/")
trends_df.write.mode("overwrite").parquet(f"{output_path}/trends/")

logger.info("Construyendo tablas dim y fact...")
### ------------------ Construcción de tablas dim y fact ------------------ ###

# Tabla 1: dim_news_source
dim_news_source_df = cleaned_df.select("name").distinct() \
    .withColumn("source_id", row_number().over(Window.orderBy("name"))) \
    .withColumn("reliability_score", lit(0.8))

dim_news_source_df.write.mode("overwrite").parquet(f"{output_path}/dim_news_source/")

# Tabla 2: dim_topic
dim_topic_df = keywords_df.select(
    col("keywords").alias("name")
).distinct().withColumn("topic_id", row_number().over(Window.orderBy("name"))) \
  .withColumn("category", lit("general"))

dim_topic_df.write.mode("overwrite").parquet(f"{output_path}/dim_topic/")

# Tabla 3: fact_article
fact_article_df = cleaned_df.join(dim_news_source_df, "name", "left") \
    .withColumn("article_id", row_number().over(Window.orderBy("published_date"))) \
    .withColumn("topic_id", lit(None).cast("int"))  # Placeholder para topic_id

fact_article_df.write.partitionBy("published_date").mode("overwrite").parquet(f"{output_path}/fact_article/")

logger.info("Procesamiento completado. Datos guardados en S3.")
spark.stop()
