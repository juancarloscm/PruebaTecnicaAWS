import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, explode, split, count, date_format, lit, row_number
from pyspark.sql.window import Window
import pyspark.sql.functions as F
import logging

# ConfiguraciÃ³n de logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("NewsDataProcessing")

# Crear la sesiÃ³n de Spark
spark = SparkSession.builder \
    .appName("NewsDataProcessing") \
    .getOrCreate()

# Rutas de entrada y salida
input_path = "s3://spaceflight-data-pipeline/raw"
output_path = "s3://spaceflight-data-pipeline/processed"

logger.info("ðŸ“¥ Leyendo datos desde S3...")
# Leer datos desde S3
data_df = spark.read.json(f"{input_path}/*.json")

# Extraer los datos directamente SIN `results`
cleaned_df = data_df.select(
    "id", 
    "title", 
    "summary", 
    date_format(col("published_at"), "yyyy-MM-dd").alias("published_date"),
    lower(col("news_site")).alias("name"),
    "url"
)

logger.info("ðŸ”Ž Procesando datos para anÃ¡lisis de contenido y tendencias...")
### ------------------ AnÃ¡lisis de contenido y tendencias ------------------ ###

# AnÃ¡lisis de contenido: ExtracciÃ³n de palabras clave
keywords_df = cleaned_df.withColumn("keywords", explode(split(col("summary"), "\\s+"))) \
    .groupBy("keywords") \
    .agg(count("keywords").alias("count")) \
    .orderBy(F.desc("count"))

# AnÃ¡lisis de tendencias: Conteo de artÃ­culos por fecha
trends_df = cleaned_df.groupBy("published_date") \
    .agg(count("id").alias("article_count")) \
    .orderBy("published_date")

logger.info("ðŸ’¾ Guardando anÃ¡lisis en S3 (Parquet)...")
# Guardar resultados en S3 en formato Parquet
keywords_df.write.mode("overwrite").parquet(f"{output_path}/keywords/")
trends_df.write.mode("overwrite").parquet(f"{output_path}/trends/")

logger.info("ðŸ›  Construyendo tablas dim y fact...")
### ------------------ ConstrucciÃ³n de tablas dim y fact ------------------ ###

# Tabla 1: dim_news_source
dim_news_source_df = cleaned_df.select("name").distinct() \
    .withColumn("source_id", row_number().over(Window.orderBy("name"))) \
    .withColumn("reliability_score", lit(0.8))

dim_news_source_df.write.mode("overwrite").parquet(f"{output_path}/dim_news_source/")

# Tabla 2: dim_topic
dim_topic_df = keywords_df.select(
    col("keywords").alias("name")
).distinct().withColumn("topic_id", row_number().over(Window.orderBy("name"))) \
  .withColumn("category", lit("general"))

dim_topic_df.write.mode("overwrite").parquet(f"{output_path}/dim_topic/")

# Tabla 3: fact_article (con asignaciÃ³n correcta de topic_id)
fact_article_df = cleaned_df.alias("articles") \
    .join(keywords_df.alias("keywords"), 
          F.expr("articles.summary LIKE CONCAT('%', keywords.keywords, '%')"), 
          "left") \
    .join(dim_topic_df.alias("topics"), 
          F.col("keywords.keywords") == F.col("topics.name"), 
          "left") \
    .join(dim_news_source_df.alias("sources"), 
          "name", 
          "left") \
    .select(
        F.row_number().over(Window.orderBy("published_date")).alias("article_id"),
        F.col("sources.source_id"),
        F.col("topics.topic_id"),  
        F.col("published_date"),
        F.col("title"),
        F.col("summary"),
        F.col("url")
    )

logger.info("ðŸ“¤ Guardando fact_article en S3 particionado por published_date...")
fact_article_df.write.partitionBy("published_date").mode("overwrite").parquet(f"{output_path}/fact_article/")

logger.info("âœ… Procesamiento completado. Datos guardados en S3.")
spark.stop()
