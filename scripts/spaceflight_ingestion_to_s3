import json
import boto3
import requests
from datetime import datetime
from tenacity import retry, stop_after_attempt, wait_fixed
import logging

# Configuración
BUCKET_NAME = "spaceflight-data-pipeline"
FOLDER_NAME = "raw"
API_BASE_URL = "https://api.spaceflightnewsapi.net/v4"
PAGE_SIZE = 100
TIMEOUT = 10

# Inicializar clientes de AWS
s3 = boto3.client('s3')
eventbridge = boto3.client('events')

# Configuración de logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()

# Reintento automático con Tenacity (hasta 2 intentos con 5 segundos de espera)
@retry(stop=stop_after_attempt(2), wait=wait_fixed(5))
def fetch_paginated_data(endpoint):
    """Obtiene datos paginados desde la API."""
    start = 0
    page_count = 0
    
    while True:
        url = f"{API_BASE_URL}/{endpoint}?_limit={PAGE_SIZE}&_start={start}"
        try:
            response = requests.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            page_data = response.json()

            # Verificar si 'results' está presente y no está vacío
            if "results" not in page_data or not page_data["results"]:
                logger.info(f"No hay más datos en {endpoint}. Páginas procesadas: {page_count}")
                break
            
            # Guardar solo la parte de "results" en S3
            save_to_s3(page_data["results"], f"{endpoint}_page_{start}")
            page_count += 1
            start += PAGE_SIZE

            logger.info(f"Página {page_count} de {endpoint} procesada.")
            
            # Limitar a 5 páginas para pruebas
            if page_count >= 5:
                logger.info("Se ha alcanzado el límite de 5 páginas.")
                break

        except requests.exceptions.RequestException as e:
            logger.error(f"Error al obtener datos de {url}: {str(e)}")
            break
        except (TypeError, KeyError) as e:
            logger.error(f"Error procesando datos de {endpoint} en la página {start}: {str(e)}")
            break

def save_to_s3(data, key):
    """Guarda los datos en S3 como archivo JSON."""
    try:
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        key = f"{FOLDER_NAME}/{key}_{timestamp}.json"
        s3.put_object(Bucket=BUCKET_NAME, Key=key, Body=json.dumps(data))
        logger.info(f"Datos guardados en S3: {key}")
    except Exception as e:
        logger.error(f"Error guardando datos en S3: {str(e)}")

def send_event_to_eventbridge():
    """Envía un evento a EventBridge indicando que la ingesta ha finalizado."""
    try:
        response = eventbridge.put_events(
            Entries=[
                {
                    'Source': 'spaceflight-data-ingestion',
                    'DetailType': 'IngestionCompleted',
                    'Detail': json.dumps({
                        "message": "Ingesta completada con éxito",
                        "bucket": BUCKET_NAME,
                        "path": f"s3://{BUCKET_NAME}/{FOLDER_NAME}"
                    }),
                    'EventBusName': 'default'
                }
            ]
        )
        logger.info(f"Evento enviado a EventBridge: {response}")
    except Exception as e:
        logger.error(f"Error al enviar evento a EventBridge: {str(e)}")

def lambda_handler(event, context):
    """Manejador de AWS Lambda."""
    endpoints = ["articles", "blogs", "reports"]
    
    for endpoint in endpoints:
        logger.info(f"Procesando datos de {endpoint}...")
        fetch_paginated_data(endpoint)
    
    # Enviar evento a EventBridge
    send_event_to_eventbridge()
    
    return {
        "statusCode": 200,
        "body": json.dumps("Ingestión completada y evento enviado a EventBridge.")
    }

